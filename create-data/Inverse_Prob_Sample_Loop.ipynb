{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 0\n",
      "results 0\n",
      "counts 0\n",
      "data 0\n",
      "ALL 0\n",
      "sample 1\n",
      "results 1\n",
      "counts 1\n",
      "data 1\n",
      "ALL 1\n",
      "sample 2\n",
      "results 2\n",
      "counts 2\n",
      "data 2\n",
      "ALL 2\n",
      "sample 3\n",
      "results 3\n",
      "counts 3\n",
      "data 3\n",
      "ALL 3\n",
      "sample 4\n",
      "results 4\n",
      "counts 4\n",
      "data 4\n",
      "ALL 4\n",
      "sample 5\n",
      "results 5\n",
      "counts 5\n",
      "data 5\n",
      "ALL 5\n",
      "sample 6\n",
      "results 6\n",
      "counts 6\n",
      "data 6\n",
      "ALL 6\n",
      "sample 7\n",
      "results 7\n",
      "counts 7\n",
      "data 7\n",
      "ALL 7\n",
      "sample 8\n",
      "results 8\n",
      "counts 8\n",
      "data 8\n",
      "ALL 8\n",
      "sample 9\n",
      "results 9\n",
      "counts 9\n",
      "data 9\n",
      "ALL 9\n",
      "sample 10\n",
      "results 10\n",
      "counts 10\n",
      "data 10\n",
      "ALL 10\n",
      "sample 11\n",
      "results 11\n",
      "counts 11\n",
      "data 11\n",
      "ALL 11\n",
      "sample 12\n",
      "results 12\n",
      "counts 12\n",
      "data 12\n",
      "ALL 12\n",
      "sample 13\n",
      "results 13\n",
      "counts 13\n",
      "data 13\n",
      "ALL 13\n",
      "sample 14\n",
      "results 14\n",
      "counts 14\n",
      "data 14\n",
      "ALL 14\n",
      "sample 15\n",
      "results 15\n",
      "counts 15\n",
      "data 15\n",
      "ALL 15\n",
      "sample 16\n",
      "results 16\n",
      "counts 16\n",
      "data 16\n",
      "ALL 16\n",
      "sample 17\n",
      "results 17\n",
      "counts 17\n",
      "data 17\n",
      "ALL 17\n",
      "sample 18\n",
      "results 18\n",
      "counts 18\n",
      "data 18\n",
      "ALL 18\n",
      "sample 19\n",
      "results 19\n",
      "counts 19\n",
      "data 19\n",
      "ALL 19\n",
      "sample 20\n",
      "results 20\n",
      "counts 20\n",
      "data 20\n",
      "ALL 20\n",
      "sample 21\n",
      "results 21\n",
      "counts 21\n",
      "data 21\n",
      "ALL 21\n",
      "sample 22\n",
      "results 22\n",
      "counts 22\n",
      "data 22\n",
      "ALL 22\n",
      "sample 23\n",
      "results 23\n",
      "counts 23\n",
      "data 23\n",
      "ALL 23\n",
      "sample 24\n",
      "results 24\n",
      "counts 24\n",
      "data 24\n",
      "ALL 24\n",
      "sample 25\n",
      "results 25\n",
      "counts 25\n",
      "data 25\n",
      "ALL 25\n",
      "sample 26\n",
      "results 26\n",
      "counts 26\n",
      "data 26\n",
      "ALL 26\n",
      "sample 27\n",
      "results 27\n",
      "counts 27\n",
      "data 27\n",
      "ALL 27\n",
      "sample 28\n",
      "results 28\n",
      "counts 28\n",
      "data 28\n",
      "ALL 28\n",
      "sample 29\n",
      "results 29\n",
      "counts 29\n",
      "data 29\n",
      "ALL 29\n",
      "sample 30\n",
      "results 30\n",
      "counts 30\n",
      "data 30\n",
      "ALL 30\n",
      "sample 31\n",
      "results 31\n",
      "counts 31\n",
      "data 31\n",
      "ALL 31\n",
      "sample 32\n",
      "results 32\n",
      "counts 32\n",
      "data 32\n",
      "ALL 32\n",
      "sample 33\n",
      "results 33\n",
      "counts 33\n",
      "data 33\n",
      "ALL 33\n",
      "sample 34\n",
      "results 34\n",
      "counts 34\n",
      "data 34\n",
      "ALL 34\n",
      "sample 35\n",
      "results 35\n",
      "counts 35\n",
      "data 35\n",
      "ALL 35\n",
      "sample 36\n",
      "results 36\n",
      "counts 36\n",
      "data 36\n",
      "ALL 36\n",
      "sample 37\n",
      "results 37\n",
      "counts 37\n",
      "data 37\n",
      "ALL 37\n",
      "sample 38\n",
      "results 38\n",
      "counts 38\n",
      "data 38\n",
      "ALL 38\n",
      "sample 39\n",
      "results 39\n",
      "counts 39\n",
      "data 39\n",
      "ALL 39\n",
      "sample 40\n",
      "results 40\n",
      "counts 40\n",
      "data 40\n",
      "ALL 40\n",
      "sample 41\n",
      "results 41\n",
      "counts 41\n",
      "data 41\n",
      "ALL 41\n",
      "sample 42\n",
      "results 42\n",
      "counts 42\n",
      "data 42\n",
      "ALL 42\n",
      "sample 43\n",
      "results 43\n",
      "counts 43\n",
      "data 43\n",
      "ALL 43\n",
      "sample 44\n",
      "results 44\n",
      "counts 44\n",
      "data 44\n",
      "ALL 44\n",
      "sample 45\n",
      "results 45\n",
      "counts 45\n",
      "data 45\n",
      "ALL 45\n",
      "sample 46\n",
      "results 46\n",
      "counts 46\n",
      "data 46\n",
      "ALL 46\n",
      "sample 47\n",
      "results 47\n",
      "counts 47\n",
      "data 47\n",
      "ALL 47\n",
      "sample 48\n",
      "results 48\n",
      "counts 48\n",
      "data 48\n",
      "ALL 48\n",
      "sample 49\n",
      "results 49\n",
      "counts 49\n",
      "data 49\n",
      "ALL 49\n",
      "CPU times: user 4h 4min 2s, sys: 18.5 s, total: 4h 4min 20s\n",
      "Wall time: 4h 4min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import csv\n",
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#this part is just the file that later on assigns words to CUIs\n",
    "f555 = open('/Users/christopherpan 1/Desktop/Topic Modeling Project Part 3/LEGIT_test_db_dict.txt','r')\n",
    "\n",
    "dictList = []\n",
    "for line in f555:\n",
    "    ll = line.split('|')\n",
    "    dictList.append([ll[0],ll[1]])\n",
    "f555.close()\n",
    "\n",
    "\n",
    "cui_vecs = '/Users/christopherpan 1/Desktop/Topic Modeling Project Part 4/topic model ra legit/raTags.csv'\n",
    "#tags (cosine similarity) relative to CUI\n",
    "rows = []\n",
    "with open(cui_vecs, 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    first = True\n",
    "    for row in reader:\n",
    "        if first:\n",
    "            first = False\n",
    "            continue\n",
    "        else:\n",
    "            rows.append(row)\n",
    "            \n",
    "rows.sort(key=lambda x: x[2], reverse = True)\n",
    "\n",
    "#professor wants 50 samples -> get count of each variable generated (kind of like when the CUIs were counted)\n",
    "for iiiiii in range(0,50):\n",
    "\n",
    "    sample = []\n",
    "    indexx = 0\n",
    "\n",
    "    for row in rows:\n",
    "        r = random.random()\n",
    "        if r <= float(row[2]) and float(row[2]) > 0.4:\n",
    "            sample.append([row[0],row[2]])\n",
    "            indexx += 1\n",
    "        #special case for 0.1-0.4 range: divide cos_sim by 4\n",
    "        elif float(row[2]) <= 0.4 and float(row[2]) >= 0.1 and r < float(row[2])/4:\n",
    "            sample.append([row[0],row[2]])\n",
    "            indexx += 1\n",
    "        #special case for <0.1 range: divide cos_sim by 2\n",
    "        elif r <= float(row[2])/2.0 and float(row[2]) < 0.1:\n",
    "            sample.append([row[0],row[2]])\n",
    "            indexx += 1\n",
    "    sample.insert(0,['Filename','Tag'])\n",
    "\n",
    "    path = '/Users/christopherpan 1/Desktop/Topic Modeling Project Part 5/ra/' + str(iiiiii) + '/'\n",
    "\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    \n",
    "    f = open(os.path.join(path,'raSample.csv'), 'w')\n",
    "    with f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(sample)\n",
    "    f.close()\n",
    "    \n",
    "    #these prints were just for me to see where the program is\n",
    "    #checkpoint0\n",
    "    print('sample ' + str(iiiiii))\n",
    "    \n",
    "    df = pd.read_csv(path + 'raSample.csv',sep=\",\")\n",
    "    filenames = df['Filename']\n",
    "    tag = df['Tag']\n",
    "    tagsList = []\n",
    "    for i in range(indexx):\n",
    "        tagsList.append([filenames[i],tag[i]])\n",
    "    filenamesList = []\n",
    "    for fi in filenames:\n",
    "        filenamesList.append(fi.replace('.txt',''))\n",
    "\n",
    "    f = open(os.path.join(path,'LEGIT_RESULTS.txt'), 'w')\n",
    "    \n",
    "    #checkpoint1\n",
    "    print('results ' + str(iiiiii))\n",
    "    \n",
    "    #the sample generated only has filenames, this part gets the CUI counts of only these files (cont.)\n",
    "    #from the total data file (NLP output data e.g. test_db_res.txt)\n",
    "    \n",
    "    with open('/Users/christopherpan 1/Desktop/Topic Modeling Project Part 3/test_db_res.txt','r') as file1:\n",
    "        data = file1.readlines()\n",
    "        for line in data:\n",
    "            title = line.split('|')[0]\n",
    "            if title in filenamesList:\n",
    "                a = line.replace(':YES','')\n",
    "                b = a.replace(':NO','')\n",
    "                c = b.replace(':UNCLEAR','')\n",
    "                f.write(c)\n",
    "\n",
    "    f.close()\n",
    "    theTitles = []\n",
    "    #only get CUIs with >0.1 cos_sim to disease\n",
    "    with open('/Users/christopherpan 1/Desktop/Topic Modeling Project Part 4/topic model ra/ra0.1.txt','r') as file3:\n",
    "        data1 = file3.readlines()\n",
    "        for line in data1:\n",
    "            theTitles.append(line.split(':')[0])\n",
    "            cuislol = []\n",
    "    with open('/Users/christopherpan 1/Desktop/Topic Modeling Project Part 4/topic model ra/ra0.1.txt','r') as file3:\n",
    "        data1 = file3.readlines()\n",
    "        for line in data1:\n",
    "            cuislol.append([line.split(':')[0],line.split(':')[1].replace('\\n','')])\n",
    "    theDictlist = []\n",
    "    with open(path + 'LEGIT_RESULTS.txt','r') as file2:\n",
    "        data = file2.readlines()\n",
    "        for line in data:\n",
    "            l = line.split('|')\n",
    "            cuis = l[2].split(',')\n",
    "            for cui in cuis:\n",
    "                cui = cui.replace('\\n','')\n",
    "                if cui in theDictlist:\n",
    "                    continue\n",
    "                elif cui in theTitles:\n",
    "                    theDictlist.append(cui)\n",
    "    theDictlist.sort()\n",
    "    theDictlistCUIs = []\n",
    "    for d in theDictlist:\n",
    "        for cu in cuislol:\n",
    "            if cu[0] == d:\n",
    "                theDictlistCUIs.append([cu[0],cu[1]])\n",
    "    res = open(path + 'LEGIT_RESULTS.txt','r')\n",
    "    f2 = open(os.path.join(path,'CUICountsra.txt'), 'w')\n",
    "    results = res.readlines()\n",
    "    f2.write('filename')\n",
    "    for d in theDictlistCUIs:\n",
    "        f2.write(',' + d[0]+'|'+ d[1])\n",
    "    f2.write('\\n')\n",
    "    for line in results:\n",
    "        l = line.split('|')\n",
    "        title = l[0].replace(',','')\n",
    "        CUIs = l[2].split(',')\n",
    "        f2.write(title)\n",
    "        for di in theDictlist:\n",
    "            count = 0\n",
    "            for cui in CUIs:\n",
    "                if cui.replace('\\n','') == di:\n",
    "                    count += 1\n",
    "            f2.write(',' + str(count))\n",
    "        f2.write('\\n')\n",
    "    f2.close()\n",
    "    \n",
    "    #checkpoint2\n",
    "    print('counts ' + str(iiiiii))\n",
    "    #table with counts of all CUIs generated\n",
    "    \n",
    "    df = pd.read_csv(path + 'CUICountsra.txt',sep=\",\")\n",
    "    filenamesList = []\n",
    "    filenames = df['filename']\n",
    "    for fil in filenames:\n",
    "        filenamesList.append(fil)\n",
    "    df2 = pd.read_csv(path + 'raSample.csv',sep=\",\")\n",
    "    tag = df2['Tag']\n",
    "    filnames = df2['Filename']\n",
    "    tagsList = []\n",
    "    i = 0\n",
    "    for i in range(indexx):\n",
    "        tagsList.append([filnames[i],tag[i]])\n",
    "    tagsList.sort(key=lambda x: x[0])\n",
    "    tagAppend = []\n",
    "    for tag in tagsList:\n",
    "        tagAppend.append(tag[1])\n",
    "    df.insert(1, 'Cos_Sim', pd.Series(tagAppend))\n",
    "    df.sort_values('filename')\n",
    "    df = df.sort_values('Cos_Sim',ascending=False)\n",
    "    df.to_csv(path + 'raData.csv', sep=',',header=True)\n",
    "\n",
    "    #checkpoint3\n",
    "    print('data ' + str(iiiiii))\n",
    "    #tags appended\n",
    "    \n",
    "#     # tag > 0.5 = 1, others are 0\n",
    "#     df['Bin_tag'] = (df['Cos_Sim'] > 0.5)\n",
    "#     df['Bin_tag'] = df['Bin_tag'].astype(object).replace({False:'0',True:'1'})\n",
    "#     clean_frame_train = df.drop(['filename','Cos_Sim'],axis = 1,inplace = False)\n",
    "#     clean_frame_train = clean_frame_train.drop(clean_frame_train.columns[0],axis = 1,inplace = False)\n",
    "#     y = clean_frame_train.Bin_tag\n",
    "#     X_train, X_valid, y_train, y_valid = train_test_split(clean_frame_train, y, test_size=0.20)\n",
    "    \n",
    "#     X_tr = X_train.drop(['Bin_tag'], axis=1)\n",
    "#     X_vl = X_valid.drop(['Bin_tag'], axis=1)\n",
    "    \n",
    "#     #normalize data (?)\n",
    "#     #X_tr = normalize(X_tr.as_matrix())\n",
    "#     #X_vl = normalize(X_vl.as_matrix())\n",
    "\n",
    "#     y_tr = y_train.as_matrix()\n",
    "#     y_vl = y_valid.as_matrix()\n",
    "#     regr = LogisticRegression()\n",
    "    \n",
    "#     #try C values in this list, pick one that gets best score with validation data (I think 10 is almost always picked, haven't looked at all yet)\n",
    "#     ccs = [0.001,0.01,0.1,1,10]\n",
    "#     theC = 0.001\n",
    "#     runningScore = -1\n",
    "#     for c in ccs:\n",
    "#         regr.set_params(penalty ='l1',C = c)\n",
    "#         regr.fit(X_tr,y_tr)\n",
    "#         a = regr.predict(X_vl)\n",
    "#         score = regr.score(X_vl, y_vl)\n",
    "#         if score > runningScore:\n",
    "#             runningScore = 0\n",
    "#             theC = c\n",
    "#     regr = LogisticRegression()\n",
    "#     regr.set_params(penalty = 'l1', C = theC)\n",
    "#     regr.fit(X_tr,y_tr)\n",
    "\n",
    "#     #checkpoint4\n",
    "#     print('regression ' + str(iiiiii))\n",
    "    \n",
    "#     #get coefficients\n",
    "#     coefficients = regr.coef_[0]\n",
    "#     clean_frame_train_2 = clean_frame_train.drop(['Bin_tag'], axis = 1,inplace=False)\n",
    "#     pd.Series(coefficients, index=clean_frame_train_2.columns)\n",
    "#     coefficientList = pd.Series(coefficients, index=clean_frame_train_2.columns)\n",
    "#     llll = len(coefficientList)\n",
    "#     coeffList = []\n",
    "#     for i in range(llll):\n",
    "#         if coefficientList[i] != 0.0:\n",
    "#             coeffList.append([clean_frame_train_2.columns[i],coefficientList[i]])\n",
    "#     coeffList.sort(key=lambda x: abs(float(x[1])), reverse = True)\n",
    "#     f3 = open(path + str(iiiiii) + 'tiaWords.txt','w')\n",
    "#     f3.write('Disease:Transient Ischemic Attack\\n')\n",
    "#     f3.write('C = ' + str(theC) +'\\n')\n",
    "#     for l in coeffList:\n",
    "#         ll = l[0].split('|')[0]\n",
    "#         for lin in dictList:\n",
    "#             if lin[1] == ll:\n",
    "#                 f3.write(l[0] + ',' + str(l[1]) + ' ' + lin[0] + '\\n')\n",
    "#                 break\n",
    "#     f3.close()\n",
    "\n",
    "#     #checkpoint5\n",
    "    print('ALL ' + str(iiiiii))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:50: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:51: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:53: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:54: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regression 0\n",
      "ALL 0\n",
      "data 1\n",
      "regression 1\n",
      "ALL 1\n",
      "data 2\n",
      "regression 2\n",
      "ALL 2\n",
      "data 3\n",
      "regression 3\n",
      "ALL 3\n",
      "data 4\n",
      "regression 4\n",
      "ALL 4\n",
      "data 5\n",
      "regression 5\n",
      "ALL 5\n",
      "data 6\n",
      "regression 6\n",
      "ALL 6\n",
      "data 7\n",
      "regression 7\n",
      "ALL 7\n",
      "data 8\n",
      "regression 8\n",
      "ALL 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data 9\n",
      "regression 9\n",
      "ALL 9\n",
      "data 10\n",
      "regression 10\n",
      "ALL 10\n",
      "data 11\n",
      "regression 11\n",
      "ALL 11\n",
      "data 12\n",
      "regression 12\n",
      "ALL 12\n",
      "data 13\n",
      "regression 13\n",
      "ALL 13\n",
      "data 14\n",
      "regression 14\n",
      "ALL 14\n",
      "data 15\n",
      "regression 15\n",
      "ALL 15\n",
      "data 16\n",
      "regression 16\n",
      "ALL 16\n",
      "data 17\n",
      "regression 17\n",
      "ALL 17\n",
      "data 18\n",
      "regression 18\n",
      "ALL 18\n",
      "data 19\n",
      "regression 19\n",
      "ALL 19\n",
      "data 20\n",
      "regression 20\n",
      "ALL 20\n",
      "data 21\n",
      "regression 21\n",
      "ALL 21\n",
      "data 22\n",
      "regression 22\n",
      "ALL 22\n",
      "data 23\n",
      "regression 23\n",
      "ALL 23\n",
      "data 24\n",
      "regression 24\n",
      "ALL 24\n",
      "data 25\n",
      "regression 25\n",
      "ALL 25\n",
      "data 26\n",
      "regression 26\n",
      "ALL 26\n",
      "data 27\n",
      "regression 27\n",
      "ALL 27\n",
      "data 28\n",
      "regression 28\n",
      "ALL 28\n",
      "data 29\n",
      "regression 29\n",
      "ALL 29\n",
      "data 30\n",
      "regression 30\n",
      "ALL 30\n",
      "data 31\n",
      "regression 31\n",
      "ALL 31\n",
      "data 32\n",
      "regression 32\n",
      "ALL 32\n",
      "data 33\n",
      "regression 33\n",
      "ALL 33\n",
      "data 34\n",
      "regression 34\n",
      "ALL 34\n",
      "data 35\n",
      "regression 35\n",
      "ALL 35\n",
      "data 36\n",
      "regression 36\n",
      "ALL 36\n",
      "data 37\n",
      "regression 37\n",
      "ALL 37\n",
      "data 38\n",
      "regression 38\n",
      "ALL 38\n",
      "data 39\n",
      "regression 39\n",
      "ALL 39\n",
      "data 40\n",
      "regression 40\n",
      "ALL 40\n",
      "data 41\n",
      "regression 41\n",
      "ALL 41\n",
      "data 42\n",
      "regression 42\n",
      "ALL 42\n",
      "data 43\n",
      "regression 43\n",
      "ALL 43\n",
      "data 44\n",
      "regression 44\n",
      "ALL 44\n",
      "data 45\n",
      "regression 45\n",
      "ALL 45\n",
      "data 46\n",
      "regression 46\n",
      "ALL 46\n",
      "data 47\n",
      "regression 47\n",
      "ALL 47\n",
      "data 48\n",
      "regression 48\n",
      "ALL 48\n",
      "data 49\n",
      "regression 49\n",
      "ALL 49\n"
     ]
    }
   ],
   "source": [
    "## %%time\n",
    "import csv\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import linear_model\n",
    "\n",
    "f555 = open('/Users/christopherpan 1/Desktop/Topic Modeling Project Part 3/LEGIT_test_db_dict.txt','r')\n",
    "\n",
    "dictList = []\n",
    "for line in f555:\n",
    "    ll = line.split('|')\n",
    "    dictList.append([ll[0],ll[1]])\n",
    "f555.close()\n",
    "\n",
    "for iiiiii in range(0,50):\n",
    "    path = '/Users/christopherpan 1/Desktop/Topic Modeling Project Part 5/ra/' + str(iiiiii) + '/'\n",
    "    path2 = '/Users/christopherpan 1/Desktop/Topic Modeling Project_SGD/ra/' + str(iiiiii) + '/'\n",
    "\n",
    "    if not os.path.exists(path2):\n",
    "        os.makedirs(path2)\n",
    "    \n",
    "    df = pd.read_csv(path + 'raData.csv',sep=\",\")\n",
    "    df = df.drop(df.columns[0], axis=1, inplace = False)\n",
    "    #checkpoint3\n",
    "    print('data ' + str(iiiiii))\n",
    "    #tags appended\n",
    "    \n",
    "    # tag > 0.5 = 1, others are 0\n",
    "    df['Bin_tag'] = (df['Cos_Sim'] > 0.5)\n",
    "    df['Bin_tag'] = df['Bin_tag'].astype(object).replace({False:'0',True:'1'})\n",
    "    clean_frame_train = df.drop(['filename','Cos_Sim'],axis = 1,inplace = False)\n",
    "    clean_frame_train = clean_frame_train.drop(clean_frame_train.columns[0],axis = 1,inplace = False)\n",
    "    y = clean_frame_train.Bin_tag\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(clean_frame_train, y, test_size=0.20)\n",
    "    \n",
    "    X_tr = X_train.drop(['Bin_tag'], axis=1)\n",
    "    X_vl = X_valid.drop(['Bin_tag'], axis=1)\n",
    "    \n",
    "#     numeric_cols = [col for col in X_tr if X_tr[col].dtype.kind != 'O']\n",
    "#     X_tr[numeric_cols] += 1\n",
    "#     X_tr = np.log(X_tr)\n",
    "    \n",
    "    #normalize data (?)\n",
    "    X_tr = normalize(X_tr.as_matrix())\n",
    "    X_vl = normalize(X_vl.as_matrix())\n",
    "\n",
    "    y_tr = y_train.as_matrix()\n",
    "    y_vl = y_valid.as_matrix()\n",
    "    \n",
    "    #try C values in this list, pick one that gets best score with validation data (I think 10 is almost always picked, haven't looked at all yet)\n",
    "#     ccs = [0.001,0.01,0.1,1,10]\n",
    "#     theC = 0.001\n",
    "#     runningScore = -1\n",
    "#     for c in ccs:\n",
    "#         regr.set_params(penalty ='l1',C = c)\n",
    "#         regr.fit(X_tr,y_tr)\n",
    "#         a = regr.predict(X_vl)\n",
    "#         score = regr.score(X_vl, y_vl)\n",
    "#         if score > runningScore:\n",
    "#             runningScore = 0\n",
    "#             theC = c\n",
    "#     regr = LogisticRegression()\n",
    "#     regr.set_params(penalty = 'l1', C = theC)\n",
    "#     regr.fit(X_tr,y_tr)\n",
    "    \n",
    "    regr = linear_model.SGDClassifier(loss='log', penalty='elasticnet', alpha=.001, l1_ratio=0.15)\n",
    "    regr.fit(X_tr,y_tr)\n",
    "\n",
    "    #checkpoint4\n",
    "    print('regression ' + str(iiiiii))\n",
    "    \n",
    "    #get coefficients\n",
    "    coefficients = regr.coef_[0]\n",
    "    clean_frame_train_2 = clean_frame_train.drop(['Bin_tag'], axis = 1,inplace=False)\n",
    "    pd.Series(coefficients, index=clean_frame_train_2.columns)\n",
    "    coefficientList = pd.Series(coefficients, index=clean_frame_train_2.columns)\n",
    "    llll = len(coefficientList)\n",
    "    coeffList = []\n",
    "    for i in range(llll):\n",
    "        if coefficientList[i] != 0.0:\n",
    "            coeffList.append([clean_frame_train_2.columns[i],coefficientList[i]])\n",
    "    coeffList.sort(key=lambda x: abs(float(x[1])), reverse = True)\n",
    "    f3 = open(path2 + str(iiiiii) + 'raWords.txt','w')\n",
    "    f3.write('Disease:Rheumatoid Arthritis\\n')\n",
    "    f3.write('SGDClassifier: alpha = 0.001, l1_ratio = 0.15\\n')\n",
    "    for l in coeffList:\n",
    "        ll = l[0].split('|')[0]\n",
    "        for lin in dictList:\n",
    "            if lin[1] == ll:\n",
    "                f3.write(l[0] + ',' + str(l[1]) + ' ' + lin[0] + '\\n')\n",
    "                break\n",
    "    f3.close()\n",
    "\n",
    "    #checkpoint5\n",
    "    print('ALL ' + str(iiiiii))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
